---
title: "LAB6"
author: "Caroline He"
date: "10/1/2021"
output: github_document
---

# Lab 06： Text Mining

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
```

## Learning goals
* Use unnest_tokens() and unnest_ngrams() to extract tokens and ngrams from text.
* Use dplyr and ggplot2 to analyze text data

### Setup packages
You should load in dplyr, (or data.table if you want to work that way), ggplot2 and tidytext. If you don’t already have tidytext then you can install
```{r packages setup}
library(data.table)
library(dplyr)
library(tidyverse)
library(tidytext)
library(tibble)
library(ggplot2)
```

### read in Medical Transcriptions
Loading in reference transcription samples from https://www.mtsamples.com/
```{r download and read in data}
fn <- "mtsamples.csv"
if(!file.exists(fn))
  download.file("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv", destfile = fn)

mtsamples <- read.csv(fn)
mtsamples <- as_tibble(mtsamples)
head(mtsamples)
```

## Question 1: What specialties do we have?
We can use count() from dplyr to figure out how many different catagories do we have? Are these catagories related? overlapping? evenly distributed?
```{r dist-of-specialities}
specialties <- mtsamples %>%
  count(medical_specialty)
```
There are `r nrow(specialties)` specialties

Determine the distribution
```{r}
# Method 1
ggplot(mtsamples, mapping = aes(x = medical_specialty)) +
  geom_histogram(stat = "count") +
  coord_flip()
```

```{r}
# Method 2
ggplot(specialties, aes(x = n, y = fct_reorder(medical_specialty, n))) +
  geom_col()
```

## Question 2
* Tokenize the the words in the transcription column
* Count the number of times each token appears
* Visualize the top 20 most frequent words
Explain what we see from this result. Does it makes sense? What insights (if any) do we get?
```{r token-transcript, cache = TRUE}
mtsamples %>%
  unnest_tokens(output = word, input = transcription) %>%
  count(word, sort = TRUE)%>%
  top_n(20)
#  ggplot(aes(x = n, y = fct_reorder(word, n))) +
#  geom_histogram(stat = "count") 
```
The word "patient" seems to be important, but we observe a lot of stopwords.

## Question 3
* Redo visualization but remove stopwords before
* Bonus points if you remove numbers as well
What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?
```{r}

```


## Question 4
repeat question 2, but this time tokenize into bi-grams. how does the result change if you look at tri-grams?

## Question 5
Using the results you got from questions 4. Pick a word and count the words that appears after and before it.

## Question 6
Which words are most used in each of the specialties. you can use group_by() and top_n() from dplyr to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?

## Question 7
Find your own insight in the data:

Ideas:

* Interesting ngrams
* See if certain words are used more in some specialties then others






